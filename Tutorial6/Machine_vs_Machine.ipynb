{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b8935d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "title: Machine vs Machine\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aeea48",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "::::{figure} https://upload.wikimedia.org/wikipedia/commons/3/38/Terminator_2.png\n",
    ":alt: Terminator 2\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1121413",
   "metadata": {},
   "source": [
    "```{attention}\n",
    "The deadline for this tutorial is extended by one week, i.e., after the midterm date.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebeb64e",
   "metadata": {
    "editable": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import logging\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import weka.core.jvm as jvm\n",
    "from IPython import display\n",
    "from joblib import Memory, Parallel, delayed, dump, load\n",
    "from scipy.io import arff\n",
    "from sklearn import ensemble, tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from weka.classifiers import Classifier, Evaluation, SingleClassifierEnhancer\n",
    "from weka.core.converters import Loader\n",
    "\n",
    "%matplotlib widget\n",
    "if not os.getenv(\n",
    "    \"NBGRADER_EXECUTION\"\n",
    "):\n",
    "    %reload_ext jupyter_ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1d24ff",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "Do NOT modify the following configuration as it is required to cache/load your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cfcee3",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "jvm.start(logging_level=logging.ERROR)\n",
    "# cache to private folder\n",
    "os.makedirs(\"private\", exist_ok=True)\n",
    "memory = Memory(location=\"private\", verbose=0)\n",
    "\n",
    "# To tabulate results of ensemble methods\n",
    "def tabulate(results):\n",
    "    df = pd.DataFrame(\n",
    "        columns=[f\"max_depth={max_depth}\" for max_depth in max_depth_list], dtype=float\n",
    "    )\n",
    "    df.insert(0, \"n_estimators\", n_estimators_list)\n",
    "    df.loc[:, lambda df: ~df.columns.isin([\"n_estimators\"])] = np.reshape(\n",
    "        results, (len(n_estimators_list), len(max_depth_list)), order=\"F\"\n",
    "    )\n",
    "    return df\n",
    "\n",
    "# To plot the dataframe\n",
    "def plot(df):\n",
    "    for col in df.columns[1:]:\n",
    "        plt.plot(df[\"n_estimators\"], df[col], label=col, marker=\"o\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"n_estimators\")\n",
    "    plt.ylabel(\"Accuracies\")\n",
    "\n",
    "# Load file\n",
    "def load_file(filename):\n",
    "    if os.path.exists(filename):\n",
    "        os.replace(filename, \"private/\" + filename)\n",
    "    return load(\"private/\" + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c2fba2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "In this notebook, we will strive to build the best machine to classify the image segmentation datasets:\n",
    "\n",
    "- `segment-challenge.arff` for training, and\n",
    "- `segment-test.arff` for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26607216",
   "metadata": {},
   "source": [
    "## Knowledge Flow Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e601f9a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "Weka provides a KnowledgeFlow interface to flow data through a learning algorithm. Unlike other interfaces (Explorer and Experimenter), KnowledgeFlow interface can train a classifier incrementally as more and more data are available. To have an overview of the interface, take a look at the [video tutorial](https://www.futurelearn.com/info/courses/more-data-mining-with-weka/0/steps/29106) by Witten. For more details, refer to the manual [here](https://www.cs.waikato.ac.nz/ml/weka/Witten_et_al_2016_appendix.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4ab31e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "::::{seealso} If you are interested in data stream mining, ...\n",
    ":class: dropdown\n",
    "\n",
    "- Consider exploring [MOA (Massive Online Analysis)](https://moa.cms.waikato.ac.nz/), a framework for data stream mining that is based on Weka. It is designed for massive data streams and provides a comprehensive suite of machine learning algorithms.\n",
    "    - Start a `Desktop` from the `File->New Launcher` and run `moa` in the terminal.\n",
    "- Try the python package [CapyMOA](https://capymoa.org/), which allows you to leverage MOA's capabilities within a Python environment.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a0ca1c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Open a layout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e66a3b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "1. Run Weka.\n",
    "2. Click `KnowledgeFlow` button under Applications.\n",
    "3. An untitled new layout should have been opened. You can also create new layout using `File â†’ New Layout`.\n",
    "4. To load a layout, click `File->Open...`. Try opening [segment-RF.kf](segment-RF.kf) and [segment-Adaboost.kf](segment-Adaboost.kf) in `Tutorial6` folder.\n",
    "5. To save the current layout to a new file, click the menu item `File->Save/Save As...`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b271da26",
   "metadata": {},
   "source": [
    "### Run a layout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb10e3a",
   "metadata": {},
   "source": [
    "With [segment-RF.kf](segment-RF.kf) or [segment-Adaboost.kf](segment-Adaboost.kf) opened:\n",
    "1. Click the play button to start the training/testing. Unlike the Explorer interface, we can flow data through multiple classification algorithms simultaneously.\n",
    "2. If the `Status` panel at the bottom shows a successful run, \n",
    "    - right-click any `TextViewer` block and select `show results` to show the collected result;[^save]\n",
    "    - right-click any `GraphViewer` block (e.g., in `segment-RF.kf`) and select `show plots` to show the plots.\n",
    "\n",
    "[^save]: You may save the text results to files as in the Explorer interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcd292c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "The following demonstrate how to use the interface to create a layout to train and test a J48 decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb535c9",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2308457",
   "metadata": {},
   "source": [
    "To load the training data:\n",
    "\n",
    "1. Click `ArffLoader` (under the `DataSources` folder) from the `Design` panel on the left, then click anywhere on the layout panel to add it.[^data]\n",
    "2. Similarly, add a `ClassAssigner` and a `TrainingSetMaker` (under `Evaluation` folder) to the layout.\n",
    "3. Right-click the `ArffLoader` and select `dataSet` (under `Connections:`). Click `ClassAssigner` in the layout to connect the data to it.[^class]\n",
    "4. Similarly, connect the data from the `ClassAssigner` to `TrainingSetMaker`.\n",
    "5. Right-click the `ArffLoader` and click `Browse...` to select the training data `segment-challenge.arff`.\n",
    "\n",
    "[^data]: On the JupyterHub server, you may find the data under `/data/`.\n",
    "[^class]: By default, `ClassAssigner` selects the last attribute as the target, but you can change the target attribute by double-clicking `ClassAssigner`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763bb209",
   "metadata": {},
   "source": [
    "To load the test data:\n",
    "\n",
    "1. Add another `ArffLoader` and `ClassAssigner` to the layout and connect the data from the prior to the latter. Alternatively, instead of adding the same block/connection multiple times, you can select, copy, and paste existing blocks.[^rename]\n",
    "2. Add a `TestSetMaker` (instead of a `TrainingSetMaker` under the `Evaluation` folder) and connect the data from the new `ClassAssigner` to it.\n",
    "3. Configure the new `ArffLoader` to load `segment-test.arff` (instead of `segment-challenge.arff`).\n",
    "\n",
    "[^rename]: You can rename a block to differentiate it from other blocks in the layout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a7fecf",
   "metadata": {},
   "source": [
    "### Setup the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92172870",
   "metadata": {},
   "source": [
    "1. Add a `J48` (under `Classifiers/trees` folder)[^config1] and a `ClassifierPerformanceEvaluator`[^config2] (under `Evaluation`) to the layout.\n",
    "2. Connect the `trainingSet` from `TrainingSetMaker` to `J48`.\n",
    "3. Connect the `testSet` from `TestSetMaker` to `J48`.\n",
    "4. Connect the `batchClassifer` from `J48` to `ClassifierPerformanceEvaluator`.\n",
    "\n",
    "[^config1]: You can configure the classifier by double clicking `J48`.\n",
    "[^config2]: You can configure the evaluation metrics by double clicking `ClassifierPerformanceEvaluator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9379a929",
   "metadata": {},
   "source": [
    "### Display the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb8b351",
   "metadata": {},
   "source": [
    "1. Add two `TextViewer`s and a `GraphViewer` (under `Visualization` folder) to the layout.[^single]\n",
    "2. Connect the `text` and `graph` from `J48` to the first `TextViewer` and the `GraphViewer`, respectively.\n",
    "3. Connect the `text` from the `ClassifierPerformanceEvaluator` to the second `TextViewer`.\n",
    "\n",
    "[^single]: You can also reuse the same `TextViewer` instead to receive multiple `text`s from different blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fe234e",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:1\n",
    "Using the KnowledgeFlow interface, add other classifiers: `IBk`, `ZeroR`, `OneR`, `PART`, and `JRIP`. Record their *fractional* accuracies in the dictionary `performance` as follows:\n",
    "\n",
    "```python\n",
    "performance = {'J48': 0.961728,\n",
    "               'IBk': ___,\n",
    "               'ZeroR': ___,\n",
    "               'OneR': ___,\n",
    "               'PART': ___,\n",
    "               'JRIP': ___}\n",
    "```\n",
    "\n",
    "Use the default parameters.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f6c9db",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cfcd3892a8b97653ee517e987f59f4bf",
     "grade": false,
     "grade_id": "kf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError\n",
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619ad2a7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "23f512e1e2795cc16910d33e1f48e920",
     "grade": true,
     "grade_id": "test-kf",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# hidden tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0989dda3",
   "metadata": {},
   "source": [
    "## Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20be8e59",
   "metadata": {},
   "source": [
    "Unlike training an individual classifier, ensemble methods\n",
    "- train an army of base classifiers and\n",
    "- combine their decisions into a final decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ce480f",
   "metadata": {},
   "source": [
    "We will use the ensemble methods implemented in scikit-learn and Weka. To load the data for scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce00d8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_url(url):\n",
    "    ftpstream = urllib.request.urlopen(url)\n",
    "    df = pd.DataFrame(arff.loadarff(io.StringIO(ftpstream.read().decode(\"utf-8\")))[0])\n",
    "    return df.loc[:, lambda df: ~df.columns.isin([\"class\"])], df[\"class\"].astype(str)\n",
    "\n",
    "\n",
    "weka_data_path = (\n",
    "    \"https://raw.githubusercontent.com/fracpete/wekamooc/master/dataminingwithweka/data/\"\n",
    ")\n",
    "X_train, Y_train = load_url(weka_data_path + \"segment-challenge.arff\")\n",
    "X_test, Y_test = load_url(weka_data_path + \"segment-test.arff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08796d1f",
   "metadata": {},
   "source": [
    "To load the data for `python-weka-wrapper`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4721462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Loader(classname=\"weka.core.converters.ArffLoader\")\n",
    "trainset = loader.load_url(\n",
    "    weka_data_path + \"segment-challenge.arff\"\n",
    ")  # use load_file to load from file instead\n",
    "trainset.class_is_last()\n",
    "\n",
    "testset = loader.load_url(weka_data_path + \"segment-test.arff\")\n",
    "testset.class_is_last()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2db7761",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8f54f6",
   "metadata": {},
   "source": [
    "Bagging (Bootstrap Aggregation) is a simple ensemble method that trains different base classifiers by *applying a classification algorithm to different bootstrapped datasets.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691fa264",
   "metadata": {},
   "source": [
    "For instance, the following uses the [`sklearn.ensemble.BaggingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) to train 10 decision trees with a maximum depth of 5:\n",
    "\n",
    "```python\n",
    "from sklearn import ensemble\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290b5902",
   "metadata": {},
   "outputs": [],
   "source": [
    "BAG = ensemble.BaggingClassifier(\n",
    "    estimator=tree.DecisionTreeClassifier(max_depth=5),\n",
    "    n_estimators=10,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "BAG.fit(X_train, Y_train)\n",
    "print(f\"Accuracy: {BAG.score(X_test, Y_test):.4g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36619697",
   "metadata": {},
   "source": [
    "The ensemble method can be parallelized for both training and classification by setting the additional parameter `n_jobs`, the number of jobs to run in parallel. Different jobs will be run in different CPU cores or threads. See the documentation [here](https://joblib.readthedocs.io/en/latest/parallel.html) for the implementation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8025ade",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "if not os.getenv(\n",
    "    \"NBGRADER_EXECUTION\"\n",
    "):\n",
    "    for n_jobs in [1, 2, 4, 8, -1]:\n",
    "        BAG.set_params(n_estimators=1000, verbose=1, n_jobs=n_jobs)\n",
    "        BAG.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b886e54a",
   "metadata": {},
   "source": [
    "::::{seealso} Why the speedup may not be proportional to the number of jobs?\n",
    ":class: dropdown\n",
    "\n",
    "There are memory and communication overheads required to parallelize the training. Concurrent access to the same memory can lead to unexpected behavior, so it must be prevented by duplicating or locking the data, leading to overheads.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdd61d9",
   "metadata": {},
   "source": [
    "You can check the number of CPU cores available using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bd4d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "_nproc = !nproc\n",
    "_nproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5023b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai\n",
    "I have {_nproc} CPU cores available. Explain the best choice of parameters to\n",
    "parallelize BaggingClassifier of sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d3114b",
   "metadata": {},
   "source": [
    "Next, we would like to see the effect of changing the depth and number of estimators. The following are the lists of possible values to explore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d221851",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_list = [1, 2, 3, 5, 10, 20]\n",
    "n_estimators_list = [1, 2, 3, 5, 10, 20, 30, 50, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abc4cd0",
   "metadata": {},
   "source": [
    "We will define a function `bagging(n_estimators, max_depth)` that returns the accuracy of Bagging `n_estimators` decision trees of maximum depth `max_depth`. To avoid re-training/evaluating a classifier, we additionally cache the result using `joblib.Memory`, which has been initialized in the first code cell as follows:\n",
    "\n",
    "```python\n",
    "from joblib import Memory\n",
    "import os\n",
    "\n",
    "# cache to private folder\n",
    "os.makedirs(\"private\", exist_ok=True)\n",
    "memory = Memory(location=\"private\", verbose=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edcd18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def bagging(n_estimators, max_depth):\n",
    "    BAG = ensemble.BaggingClassifier(\n",
    "        estimator=tree.DecisionTreeClassifier(max_depth=max_depth),\n",
    "        n_estimators=n_estimators,\n",
    "        random_state=0,\n",
    "    )\n",
    "    BAG.fit(X_train, Y_train)\n",
    "    return BAG.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cac1d8",
   "metadata": {},
   "source": [
    "To run `bagging` in parallel for different choices of parameters, we can use the following tools from `joblib`:\n",
    "\n",
    "```python\n",
    "from joblib import Parallel, delayed\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a611071e",
   "metadata": {},
   "source": [
    "The following will divide the work into 4 jobs to run in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731905ce",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "if not os.getenv(\n",
    "    \"NBGRADER_EXECUTION\"\n",
    "):\n",
    "    BAG_results = Parallel(n_jobs=4, verbose=1)(\n",
    "        delayed(bagging)(n_estimators, max_depth)\n",
    "        for max_depth in max_depth_list\n",
    "        for n_estimators in n_estimators_list\n",
    "    )\n",
    "_code = In[-1].rsplit('\\n', maxsplit=1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be3a37b",
   "metadata": {},
   "source": [
    "::::{tip}\n",
    "\n",
    "- The argument to `Parallel(n_jobs=4, verbose=1)` uses a generator expression that generates each call to `bagging` one by one. For more details, see the documentation [here](https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html).\n",
    "- The reason for `delayed(bagging)(n_estimators, max_depth)` is to delay the call of `bagging(n_estimators, max_depth)` until `Parallel(n_jobs=4, verbose=1)(...)` assigns it to a job/core/thread.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7766ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai\n",
    "Explain how generator and decorator are used in the code:\n",
    "--\n",
    "{_code}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c149b2a",
   "metadata": {},
   "source": [
    "To present the result nicely in a `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4ae407",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "BAG_df = tabulate(BAG_results)\n",
    "display.display(BAG_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4771c58d",
   "metadata": {},
   "source": [
    "Although the above calls `bagging` again for the same combinations of arguments, the cached results are returned without re-training the classifiers. You can clear the cache with the following:\n",
    "\n",
    "```python\n",
    "bagging.clear()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c20672a",
   "metadata": {},
   "source": [
    "It is helpful to save the `DataFrame` to a particular file. This can be done using [`joblib.dump`](https://joblib.readthedocs.io/en/latest/generated/joblib.dump.html):\n",
    "\n",
    "```python\n",
    "from joblib import dump\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a1a765",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "if input(\"(over-)write file? [Y/n] \").lower() != \"n\":\n",
    "    dump(BAG_df, \"BAG_df.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701813c3",
   "metadata": {},
   "source": [
    "Unlike caching, we can load the data anywhere beyond this notebook:\n",
    "\n",
    "```python\n",
    "from joblib import load\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6704c1",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "BAG_df = load(\"BAG_df.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67f1dfa",
   "metadata": {},
   "source": [
    "To remove the file:\n",
    "\n",
    "```python\n",
    "os.remove(\"BAG_df.gz\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77f8d04",
   "metadata": {},
   "source": [
    "To plot the `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bd2ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(num=1, clear=True)\n",
    "plot(BAG_df)\n",
    "plt.title(r\"Bagging decision trees\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdb1bd5",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:2\n",
    "What happens to the accuracy as `n_estimators` and `max_depth` increase?\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b094e47",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e88504de7fc48187b2c2820a3ed123f",
     "grade": true,
     "grade_id": "bag",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5759824d",
   "metadata": {},
   "source": [
    "To apply the ensemble method using `python-weka-wrapper` instead of scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd8da4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPTree = Classifier(classname=\"weka.classifiers.trees.REPTree\")\n",
    "REPTree.options = [\"-L\", \"5\"]\n",
    "BAG_weka = SingleClassifierEnhancer(classname=\"weka.classifiers.meta.Bagging\")\n",
    "BAG_weka.options = [\"-I\", \"10\", \"-S\", \"1\"]\n",
    "BAG_weka.classifier = REPTree\n",
    "BAG_weka.build_classifier(trainset)\n",
    "evl = Evaluation(testset)\n",
    "evl.test_model(BAG_weka, testset)\n",
    "print(f\"Accuracy: {evl.percent_correct/100:.4g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8161a5ef",
   "metadata": {},
   "source": [
    "The base classifiers for [`Bagging`](https://weka.sourceforge.io/doc.dev/weka/classifiers/meta/Bagging.html) are trained using [`REPTree`](https://weka.sourceforge.io/doc.dev/weka/classifiers/trees/REPTree.html), which is a fast decision tree induction algorithm that is neither C4.5 nor CART."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c5388f",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:3\n",
    "Complete the pandas `DataFrame` `BAG_weka_df` in the following cell by filling in the accuracies (as floating point numbers) for different `n_estimators` and `max_depth`.\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3c2472",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bef75dfeb7046b07d66586540f0698c1",
     "grade": false,
     "grade_id": "bag-weka",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "max_depth_list = [1, 2, 3, 5, 10, 20]\n",
    "n_estimators_list = [1, 2, 3, 5, 10, 20, 30, 50, 100]\n",
    "\n",
    "if input(\"execute? [Y/n] \").lower() != \"n\":\n",
    "    BAG_weka_df = pd.DataFrame(\n",
    "        columns=[f\"max_depth={max_depth}\" for max_depth in max_depth_list], dtype=float\n",
    "    )\n",
    "    BAG_weka_df.insert(0, \"n_estimators\", n_estimators_list)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "\n",
    "    display.display(BAG_weka_df.round(4))\n",
    "\n",
    "    plt.figure()\n",
    "    plot(BAG_weka_df)\n",
    "    plt.title(r\"Bagging decision trees\")\n",
    "    plt.show()\n",
    "\n",
    "    dump(BAG_weka_df, \"BAG_weka_df.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec78bcdc",
   "metadata": {},
   "source": [
    "::::{caution}\n",
    "\n",
    "To avoid re-training, the last line above saves your result to a file called `BAG_weka_df.gz`. Otherwise, the server cannot auto-grade your submission as it aborts execution that takes excessive time or memory. Ensure that your code is indented correctly, so it is part of the body of the conditional in the solution cell:\n",
    "```python\n",
    "...\n",
    "if input('execute? [Y/n] ').lower() != 'n':\n",
    "    ...\n",
    "```\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599d8619",
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc116e51a2633a139428ed51799957e0",
     "grade": true,
     "grade_id": "test-bag-weka",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# hidden tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3b9957",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372b1069",
   "metadata": {},
   "source": [
    "Another ensemble method, called [random forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), is similar to Bagging decision trees. However, it randomly selects or combines features to further diversify the base classifiers before building each tree. The following trains a random forest of 10 decision trees with a maximum depth of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df28101",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = ensemble.RandomForestClassifier(max_depth=5, n_estimators=10, random_state=0)\n",
    "RF.fit(X_train, Y_train)\n",
    "print(f\"Accuracy: {RF.score(X_test, Y_test):.4g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aec5d7",
   "metadata": {},
   "source": [
    "Like Bagging, we can also parallelize the training and classification by setting the `n_jobs` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8a03b4",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:4\n",
    "Complete the pandas `DataFrame` `RF_df` in the following cell by filling in the accuracies (as floating point numbers) for different `n_estimators` and `max_depth`, and with `random_state = 0`.\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be03f77",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "45a51254a695a1bdd6510d6b0f3a4e9f",
     "grade": false,
     "grade_id": "rf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "max_depth_list = [1, 2, 3, 5, 10, 20]\n",
    "n_estimators_list = [1, 2, 3, 5, 10, 20, 30, 50, 100, 200, 300]\n",
    "\n",
    "if input(\"execute? [Y/n] \").lower() != \"n\":\n",
    "    RF_df = pd.DataFrame(\n",
    "        columns=[f\"max_depth={max_depth}\" for max_depth in max_depth_list], dtype=float\n",
    "    )\n",
    "    RF_df.insert(0, \"n_estimators\", n_estimators_list)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "    display.display(RF_df.round(4))\n",
    "\n",
    "    plt.figure()\n",
    "    plot(RF_df)\n",
    "    plt.title(r\"Random forest\")\n",
    "    plt.show()\n",
    "\n",
    "    dump(RF_df, \"RF_df.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51963d83",
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "23e3c4025450d27546c6d0f7ac181621",
     "grade": true,
     "grade_id": "test-rf",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# hidden tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8f4c52",
   "metadata": {},
   "source": [
    "To train a random forest of 10 decision trees with a maximum depth of 5 using `python-weka-wrapper`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c1355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_weka = Classifier(classname=\"weka.classifiers.trees.RandomForest\")\n",
    "RF_weka.options = [\"-I\", \"10\", \"-depth\", \"5\", \"-S\", \"1\"]\n",
    "RF_weka.build_classifier(trainset)\n",
    "evl = Evaluation(testset)\n",
    "evl.test_model(RF_weka, testset)\n",
    "print(f\"Accuracy {evl.percent_correct/100:.4g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d4ae52",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:5\n",
    "Repeat the previous exercise but with [Weka](https://weka.sourceforge.io/doc.dev/weka/classifiers/trees/RandomForest.html) instead. Use a random seed of 1.\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5c7cf3",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0be8982090fbb4e6a9e16f8dfc765460",
     "grade": false,
     "grade_id": "rf-weka",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "max_depth_list = [1, 2, 3, 5, 10, 20]\n",
    "n_estimators_list = [1, 2, 3, 5, 10, 20, 30, 50, 100, 200, 300]\n",
    "\n",
    "if input(\"execute? [Y/n] \").lower() != \"n\":\n",
    "    RF_weka_df = pd.DataFrame(\n",
    "        columns=[f\"max_depth={max_depth}\" for max_depth in max_depth_list], dtype=float\n",
    "    )\n",
    "    RF_weka_df.insert(0, \"n_estimators\", n_estimators_list)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "    display.display(RF_weka_df.round(4))\n",
    "\n",
    "    plt.figure()\n",
    "    plot(RF_weka_df)\n",
    "    plt.title(r\"Random forest\")\n",
    "    plt.show()\n",
    "\n",
    "    dump(RF_weka_df, \"RF_weka_df.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc451f2",
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69923426408e6f1e47c2cf958f3bb1a5",
     "grade": true,
     "grade_id": "test-rf-weka",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# hidden tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035eabce",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:6\n",
    "What are the values of `n_estimators` and `max_depth` that maximize the above accuracy on the test set?\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288a2337",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "23969d8ed907bcab86313621f2e0383a",
     "grade": true,
     "grade_id": "rf-best",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e69ec2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "For `sklearn`, we can tune the parameters of a classification algorithm using `GridSearchCV` imported as follows:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "```\n",
    "\n",
    "For instance, to tune `n_estimators` by searching for the best value from `n_estimators_list` that maximizes the cross-validated accuracy on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42927cdf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "if input(\"execute? [Y/n] \").lower() != \"n\":\n",
    "    max_depth_list = [1, 2, 3, 5, 10, 20]\n",
    "    n_estimators_list = [1, 2, 3, 5, 10, 20, 30, 50, 100, 200, 300]\n",
    "    param_grid = {\"n_estimators\": n_estimators_list, \"max_depth\": max_depth_list}\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        ensemble.RandomForestClassifier(random_state=0), param_grid, verbose=1, n_jobs=4\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "\n",
    "    print(f\"Accuracy: {grid_search.score(X_test, Y_test):.4g}\")\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c6eebb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "::::{exercise}\n",
    ":label: ex:7\n",
    "Observe that the best parameter values that maximize the cross-validated accuracy need not maximize the accuracy on the test set. Explain whether it is better to use the maximum accuracy on the test set as the performance estimate of random forest with parameter tuning.\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e232f5b",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "336aded134c9923168791d9525d47565",
     "grade": true,
     "grade_id": "bias",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a67194",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713bab00",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "Using [AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html), we can boost the performance by adding base classifiers one-by-one to improve the error made by previously trained classifiers. To train AdaBoost with 10 decision trees of a maximum depth of 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444cf5e3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "ADB = ensemble.AdaBoostClassifier(\n",
    "    estimator=tree.DecisionTreeClassifier(max_depth=5),\n",
    "    n_estimators=10,\n",
    "    random_state=0,\n",
    ")\n",
    "ADB.fit(X_train, Y_train)\n",
    "print(f\"Accuracy: {ADB.score(X_test, Y_test):.4g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6b6e7c",
   "metadata": {},
   "source": [
    "::::{note}\n",
    "\n",
    "The original AdaBoost only works for binary classification. `AdaBoostClassifier` implements the multi-class extension of AdaBoost called AdaBoost-SAMME (Stagewise Additive Modeling using a Multi-class Exponential loss function). See Algorithm 2 of [(Zhu et al., 2009)](https://dx.doi.org/10.4310/SII.2009.v2.n3.a8).\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed6f02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai\n",
    "Explain how AdaBoost-SAMME extends AdaBoostClassifier to multi-class extension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99de0363",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:8\n",
    "Unlike Bagging and random forest, the training of the base classifiers for AdaBoost cannot be parallelized. Why?\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d94e1e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a5aa2d2cb9571df93fad853043012639",
     "grade": true,
     "grade_id": "adb-parallelize",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f63004",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:9\n",
    "Complete the pandas `DataFrame` `ADB_df` in the following cell by filling in the accuracies (as floating point numbers) for different `n_estimators` and `max_depth`. Use `random_state = 0`.\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcdf6c0",
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d65c91a0b6f3288d1f7c6d69753a15b",
     "grade": false,
     "grade_id": "adb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "max_depth_list = [1, 2, 3, 5, 10]\n",
    "n_estimators_list = [1, 2, 3, 5, 10, 20, 30, 50, 100]\n",
    "\n",
    "if input(\"execute? [Y/n] \").lower() != \"n\":\n",
    "    ADB_df = pd.DataFrame(\n",
    "        columns=[f\"max_depth={max_depth}\" for max_depth in max_depth_list], dtype=float\n",
    "    )\n",
    "    ADB_df.insert(0, \"n_estimators\", n_estimators_list)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "    display.display(ADB_df.round(4))\n",
    "\n",
    "    plt.figure()\n",
    "    plot(ADB_df)\n",
    "    plt.title(r\"Adaboost decision trees\")\n",
    "    plt.show()\n",
    "\n",
    "    dump(ADB_df, \"ADB_df.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea307cbc",
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec8f5d9d69f1146a250ef5fb33d5d1af",
     "grade": true,
     "grade_id": "test-adb",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# hidden tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b227b70a",
   "metadata": {},
   "source": [
    "To train [AdaBoost](https://weka.sourceforge.io/doc.dev/weka/classifiers/meta/AdaBoostM1.html) with 10 decision trees of maximum depth 5 using `python-weka-wrapper`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f95d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPTree = Classifier(classname=\"weka.classifiers.trees.REPTree\")\n",
    "REPTree.options = [\"-L\", \"5\"]\n",
    "ADB_weka = SingleClassifierEnhancer(classname=\"weka.classifiers.meta.AdaBoostM1\")\n",
    "ADB_weka.options = [\"-I\", \"10\", \"-S\", \"1\"]\n",
    "ADB_weka.classifier = REPTree\n",
    "ADB_weka.build_classifier(trainset)\n",
    "evl = Evaluation(testset)\n",
    "evl.test_model(ADB_weka, testset)\n",
    "print(f\"Accuracy {evl.percent_correct/100:.4g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cd1e1d",
   "metadata": {},
   "source": [
    "Weka uses the multi-class extension called AdaboostM1, which is different from Adaboost-SAMME."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fb9af3",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:10\n",
    "Repeat the previous exercise but with Weka instead. Use a random seed of 1.\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728e436a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed1c79b43dc85c3827d788f481ffb866",
     "grade": false,
     "grade_id": "adb-weka",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "if input(\"execute? [Y/n] \").lower() != \"n\":\n",
    "    max_depth_list = [1, 2, 3, 5, 10]\n",
    "    n_estimators_list = [1, 2, 3, 5, 10, 20, 30, 50, 100]\n",
    "    ADB_weka_df = pd.DataFrame(\n",
    "        columns=[f\"max_depth={max_depth}\" for max_depth in max_depth_list], dtype=float\n",
    "    )\n",
    "    ADB_weka_df.insert(0, \"n_estimators\", n_estimators_list)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "    display.display(ADB_weka_df.round(4))\n",
    "\n",
    "    plt.figure()\n",
    "    plot(ADB_weka_df)\n",
    "    plt.title(r\"Adaboost decision trees\")\n",
    "    plt.show()\n",
    "\n",
    "    dump(ADB_weka_df, \"ADB_weka_df.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d30197",
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5705cb1c9977a72ba13ebb3b7a17358d",
     "grade": true,
     "grade_id": "test-adb-weka",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# hidden tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74615b4",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:11\n",
    "\n",
    "Based on the results above, which ensemble method is better, Adaboost or random forest? Why?\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3d16c7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e676cdf3cbec4928fa23ca7795ce1e09",
     "grade": true,
     "grade_id": "rf-vs-adb",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c07f05",
   "metadata": {},
   "source": [
    "### Other methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38c1491",
   "metadata": {},
   "source": [
    "::::{exercise} optional[^optional]\n",
    ":label: ex:12\n",
    "\n",
    "Train your own classifier to achieve the highest possible accuracy. You may:\n",
    "- choose different classification algorithms or ensemble methods such as Bagging, Stacking, Voting, and XGBoost.\n",
    "- tune the hyper-parameters manually or automatically using `GridSearchCV` in scikit-learn or `CVParameterSelection` in Weka.\n",
    "\n",
    "Post your model and results on [Canvas](https://canvas.cityu.edu.hk/courses/67502/discussion_topics/597572) to compete with others. To include your code in this notebook, make sure you avoid excessive time or memory by putting your code in the body of the conditional `if input('execute? [Y/n] ').lower() != 'n':`.\n",
    "\n",
    "::::\n",
    "\n",
    "[^optional]: This exercise is optional and does not count towards the grade."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3a57bf",
   "metadata": {},
   "source": [
    "The following is an example using XGBoost with its default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000b1d7b",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "if input(\"execute? [Y/n] \").lower() != \"n\":\n",
    "    # NOTE: Restart the kernel after installing xgboost\n",
    "    %pip install xgboost\n",
    "    import xgboost\n",
    "    codes, uniques = pd.concat([Y_train, Y_test]).factorize()\n",
    "    Y_train_codes, Y_test_codes = codes[:len(Y_train)], codes[len(Y_train):]\n",
    "    XGB = xgboost.XGBClassifier(n_jobs=1)\n",
    "    XGB.fit(X_train, Y_train_codes)\n",
    "    print(f\"Accuracy: {XGB.score(X_test, Y_test_codes):.4g}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "default_lexer": "ipython3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
