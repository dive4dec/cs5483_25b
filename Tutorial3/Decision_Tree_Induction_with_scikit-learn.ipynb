{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1da527ca",
   "metadata": {},
   "source": [
    "---\n",
    "title: Decision Tree Induction with scikit-learn\n",
    "math: \n",
    "    '\\abs': '\\left\\lvert #1 \\right\\rvert' \n",
    "    '\\norm': '\\left\\lvert #1 \\right\\rvert' \n",
    "    '\\Set': '\\left\\{ #1 \\right\\}'\n",
    "    '\\mc': '\\mathcal{#1}'\n",
    "    '\\M': '\\boldsymbol{#1}'\n",
    "    '\\R': '\\mathsf{#1}'\n",
    "    '\\RM': '\\boldsymbol{\\mathsf{#1}}'\n",
    "    '\\op': '\\operatorname{#1}'\n",
    "    '\\E': '\\op{E}'\n",
    "    '\\d': '\\mathrm{\\mathstrut d}'\n",
    "    '\\Gini': '\\operatorname{Gini}'\n",
    "    '\\Info': '\\operatorname{Info}'\n",
    "    '\\Gain': '\\operatorname{Gain}'\n",
    "    '\\GainRatio': '\\operatorname{GainRatio}'\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9d32e9",
   "metadata": {
    "init_cell": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets, tree\n",
    "\n",
    "%matplotlib widget\n",
    "if not os.getenv(\n",
    "    \"NBGRADER_EXECUTION\"\n",
    "):\n",
    "    %reload_ext jupyter_ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d8ad1f",
   "metadata": {},
   "source": [
    "## Decision Tree Induction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109aae3c",
   "metadata": {},
   "source": [
    "In this notebook, we will use scikit-learn to build decision trees on the [*iris dataset*](https://en.wikipedia.org/wiki/Iris_flower_data_set) from [`sklearn.datasets`](https://scikit-learn.org/stable/api/sklearn.datasets.html#module-sklearn.datasets) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e80ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f020ac4",
   "metadata": {},
   "source": [
    "Recall the classification task is to train a model that can automatically classify the species (*target*) based on the lengths and widths of the petals and sepals (*input features*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7880bb95",
   "metadata": {},
   "source": [
    "To build a decision tree, we can use `DecisionTreeClassifier` from `sklearn.tree` and apply its `fit` method on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c771504",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gini = tree.DecisionTreeClassifier(random_state=0).fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7d6417",
   "metadata": {},
   "source": [
    "To display the decision tree, we can use the function `plot_tree` from `sklearn.tree`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6d99fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(1)\n",
    "plt.figure(num=1, clear=True)\n",
    "tree.plot_tree(clf_gini)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc3ec0d",
   "metadata": {},
   "source": [
    "::::{tip} If the figure is too small...\n",
    "\n",
    "Drag the arrow at the bottom right of the figure to resize the bounding box. \n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8b5a6a",
   "metadata": {},
   "source": [
    "For each node:\n",
    "- `___ <= ___` is the splitting criterion for internal nodes, satisfied only by samples going left.\n",
    "- `gini = ...` shows the impurity index. By default, the algorithm uses the Gini impurity index to find the best binary split. Observe that the index decreases down the tree towards the leaves.\n",
    "- `value = [_, _, _]` shows the number of associated instances for each of the three classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c397e102",
   "metadata": {},
   "source": [
    "::::{tip} How is the tree stored?\n",
    "\n",
    "The information of the decision is stored in the `tree_` attribute of the classifier. For more details, run:\n",
    "\n",
    "```python\n",
    "help(clf_gini.tree_)\n",
    "```\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a820f37",
   "metadata": {},
   "source": [
    "Additional options may be provided to customize the look of the decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a21ceae",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    \"feature_names\": iris.feature_names,\n",
    "    \"class_names\": iris.target_names,\n",
    "    \"label\": \"root\",\n",
    "    \"filled\": True,\n",
    "    \"node_ids\": True,\n",
    "    \"proportion\": True,\n",
    "    \"rounded\": True,\n",
    "    \"fontsize\": 7,\n",
    "}  # store options as dict for reuse\n",
    "plt.close(2)\n",
    "plt.figure(num=2, clear=True, figsize=(9, 9))\n",
    "tree.plot_tree(clf_gini, **options)  # ** unpacks dict as keyword arguments\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1233536e",
   "metadata": {},
   "source": [
    "Each node now indicates the majority class, which may be used as the decision. The majority classes are also color coded. Observe that the color gets lighter towards the root as the class distribution becomes more impure. In particular, the iris setosa is distinguished immediately after checking the petal width/length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b583a7a4",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:clf_entropy\n",
    "\n",
    "Assign to `clf_entropy` the decision tree classifier created using *entropy* as the impurity measure. You can do so with the keyword argument `criterion='entropy'` in `DecisionTreeClassifier`. Furthermore, Use `random_state=0` and fit the classifier on the entire iris dataset. Check whether the resulting decision tree is the same as the one created using the Gini impurity index.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd3768d",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92eed96a6520422fdb79743976836127",
     "grade": false,
     "grade_id": "tree-entropy",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError\n",
    "plt.close(3)\n",
    "plt.figure(num=3, clear=True, figsize=(9, 9))\n",
    "tree.plot_tree(clf_entropy, **options)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de981cfd",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b33243c1128464d72060ac89b3646dd1",
     "grade": true,
     "grade_id": "same-tree-as-gini",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986ba569",
   "metadata": {},
   "source": [
    "::::{caution}\n",
    "\n",
    "Although one can specify whether to use Gini impurity or entropy, `sklearn` implements neither [C4.5](https://en.wikipedia.org/wiki/C4.5_algorithm) nor [Classification and Regression Tree (CART)](https://en.wikipedia.org/wiki/Decision_tree_learning#Decision_tree_types). It supports only binary splits on numeric input attributes. See a workaround [here][categorical].\n",
    "\n",
    "[categorical]: https://stackoverflow.com/questions/38108832/passing-categorical-data-to-sklearn-decision-tree\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5d2a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai\n",
    "Explain in a paragraph what one-hot encoding is, and why it is suitable for\n",
    "converting categorical features to numeric values for scikit-learn decision trees,\n",
    "which do not support categorical data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c9fa61",
   "metadata": {},
   "source": [
    "## Splitting Criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45a14c8",
   "metadata": {},
   "source": [
    "To induce a good decision tree efficiently, the splitting criterion is chosen \n",
    "- greedily to maximize the reduction in impurity and \n",
    "- recursively starting from the root."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f89aa6",
   "metadata": {},
   "source": [
    "### Overview using pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8433f1",
   "metadata": {},
   "source": [
    "To have a rough idea of what are good features to split on, we will use [pandas](https://pandas.pydata.org/docs/user_guide/index.html) [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html?highlight=dataframe#pandas.DataFrame) \n",
    "to operate on the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c13c43",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "9gQINfrjsb4M",
    "outputId": "77b77a38-2712-4c93-c503-219e74f354fd",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# write the input features first\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "\n",
    "# append the target values to the last column\n",
    "df[\"target\"] = iris.target\n",
    "df.target = df.target.astype(\"category\").cat.rename_categories(\n",
    "    dict(zip(range(3), iris.target_names))\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ffa774",
   "metadata": {},
   "source": [
    "To display some statistics of the input features for different classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45f3685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "plt.close(4)\n",
    "fig = plt.figure(num=4, clear=True, figsize=(9, 9))\n",
    "axs = fig.subplots(1, 3, sharex=False, sharey=True)\n",
    "\n",
    "# Suppress that specific UserWarning due to an issue of Pandas\n",
    "# https://stackoverflow.com/questions/55180754/pandas-scatter-matrix\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", UserWarning)\n",
    "    df.groupby(\"target\", observed=False).boxplot(ax=axs, rot=45, fontsize=8)\n",
    "\n",
    "plt.subplots_adjust(bottom=0.25)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678c2d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"target\", observed=False).agg([\"mean\", \"std\"]).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1a746a",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:good_features\n",
    "\n",
    "Identify good feature(s) based on the above statistics. Does your choice agree with the decision tree generated by `DecisionTreeClassifier`?\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfbfdb9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "23015178e587e6f2ad5fd85735da2221",
     "grade": true,
     "grade_id": "good-features",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcafbad2",
   "metadata": {},
   "source": [
    "### Measuring impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a632f08",
   "metadata": {},
   "source": [
    "Suppose nearly all instances of a dataset belong to the same class. In that case, we can return the majority class as the decision without further splitting. A measure of impurity is the Gini impurity index, defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64fc698",
   "metadata": {},
   "source": [
    "::::{prf:definition} Gini impurity index\n",
    ":label: def:gini\n",
    "\n",
    "Given a dataset $D$ with a class attribute (discrete target), the Gini impurity index is defined as\n",
    "\n",
    "$$\n",
    "\\Gini(D):= g(p_0,p_1,\\dots)\n",
    "$$ (Gini)\n",
    "\n",
    "where $(p_0,p_1,\\dots)$ are probability masses corresponding to the empirical class distribution of $D$, and\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g(p_0,p_1,\\dots) &:= \\sum_k p_k(1-p_k)\\\\\n",
    "&= 1- \\sum_k p_k^2.\n",
    "\\end{align}\n",
    "$$ (g)\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad4cb1a",
   "metadata": {},
   "source": [
    "::::{note} For convenience, we may also write ...\n",
    "\n",
    "- $g(\\boldsymbol{p})$ for the stochastic vector $\\boldsymbol{p}=\\begin{bmatrix}p_0 & p_1 & \\dots\\end{bmatrix}$ of probability masses, and\n",
    "- $g(p)$ for the probability mass function $p: k \\mapsto p_k$.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1137db7b",
   "metadata": {},
   "source": [
    "We can represent a distribution simply as a NumPy array. To return the empirical class distributions of the iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2388ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(values):\n",
    "    \"\"\"\n",
    "    Compute the empirical distribution of the given 1D array of values.\n",
    "\n",
    "    This function calculates the empirical distribution of the input array,\n",
    "    returning a 1D array of probabilities. The order of the probabilities in\n",
    "    the output array is immaterial.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    values : array-like\n",
    "        A 1D array of values for which the empirical distribution is to be \n",
    "        computed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    probabilities : ndarray\n",
    "        A 1D array of probabilities corresponding to the empirical distribution\n",
    "        of the input values.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> values = np.array([1, 2, 2, 3, 3, 3])\n",
    "    >>> dist(values)\n",
    "    array([0.16666667, 0.33333333, 0.5       ])\n",
    "    \"\"\"\n",
    "    counts = np.unique(values, return_counts=True)[-1]\n",
    "    return counts / counts.sum()\n",
    "\n",
    "\n",
    "print(f\"Distribution of target: {dist(iris.target).round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a756667f",
   "metadata": {},
   "source": [
    "The Gini impurity index can be implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfbcf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(p):\n",
    "    \"\"\"Returns the Gini impurity of the distribution p.\"\"\"\n",
    "    return 1 - (p**2).sum()\n",
    "_code = In[-1].rsplit(maxsplit=1)[0] # store the code for chatting with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8e4136",
   "metadata": {},
   "outputs": [],
   "source": [
    "g?\n",
    "print(f\"Gini(D) = {g(dist(iris.target)):.3g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd54919",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai\n",
    "Improve the docstring to conform to NumPy style:\n",
    "--\n",
    "{_code}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36093071",
   "metadata": {},
   "source": [
    "::::{seealso} How to document your code?\n",
    "\n",
    "Properly documenting code ensures that others can reuse the code and \n",
    "collaborate to improve it. The NumPy style is a popular format for writing docstrings, which are string literals included in the code to document their usage. \n",
    "\n",
    "A more comprehensive documentation can be created using Sphinx, which includes the autodoc and Napolean extensions that can automatically extract, parse, and generate documentation from docstrings in NumPy style. To try it out, see the [documentation of Sphinx](https://www.sphinx-doc.org/en/master/usage/extensions/napoleon.html).\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5121828d",
   "metadata": {},
   "source": [
    "Another measure of impurity uses the information measure called entropy in information theory:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20f8b9d",
   "metadata": {},
   "source": [
    "::::{prf:definition} entropy\n",
    ":label: def:entropy\n",
    "\n",
    "The information content is defined as \n",
    "\n",
    "$$\n",
    "\\Info(D):= h(p_0,p_1,\\dots),\n",
    "$$ (Info)\n",
    "\n",
    "which is the *entropy* of the class distribution\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h(\\boldsymbol{p}) = h(p_0,p_1,\\dots) &= \\sum_{k:p_k>0} p_k \\log_2 \\frac1{p_k}.\n",
    "\\end{align}\n",
    "$$ (h)\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aa17bf",
   "metadata": {},
   "source": [
    "::::{prf:remark}\n",
    "\n",
    "- For convenience, we often omit the constraint $p_k>0$ by regarding $0 \\log \\frac10$ as the limit $\\lim_{p\\to 0} p \\log \\frac1{p} = 0$.\n",
    "- Unless otherwise specified, all the logarithm is base 2, i.e., $\\log = \\lg$, where the information quantities are in the unit of bit (binary digit). A popular alternative is to use the natural logarithm, $\\log = \\ln$, where the unit is in *nat*.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6152e99",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:h\n",
    "\n",
    "Complete the following function to compute the entropy of a distribution. You may use the function `log2` from `numpy` to calculate the logarithm base 2.\n",
    "\n",
    ":::{hint}\n",
    ":class: dropdown\n",
    "\n",
    "Consider the solution template:\n",
    "\n",
    "```python\n",
    "def h(p):\n",
    "    ...\n",
    "    return (p * ___ * ___).sum()\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea78ede",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "242e1425d0b39eeae6ebdcc8737fd30a",
     "grade": false,
     "grade_id": "entropy",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "def h(p):\n",
    "    # Improve the docstring below to conform to NumPy style.\n",
    "    \"\"\"Returns the entropy of distribution p (1D array).\"\"\"\n",
    "    p = np.array(p)\n",
    "    p = p[(p > 0) & (p < 1)]  # 0 log 0 = 1 log 1 = 0\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "h?\n",
    "print(f\"Info(D): {h(dist(iris.target)):.3g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c41825",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "10416def596cc71da7effe5c3946e7a1",
     "grade": true,
     "grade_id": "test-entropy",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-output",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# tests\n",
    "assert np.isclose(h([1 / 2, 1 / 2]), 1)\n",
    "assert np.isclose(h([1, 0]), 0)\n",
    "assert np.isclose(h([1 / 2, 1 / 4, 1 / 4]), 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb83d103",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75c7dda125858a45852837a19a595057",
     "grade": true,
     "grade_id": "htest-entropy",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# hidden tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63907982",
   "metadata": {},
   "source": [
    "### Drop in impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977814ea",
   "metadata": {},
   "source": [
    "The CART algorithm uses the drop in Gini impurity to select splitting attributes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2856ba4a",
   "metadata": {},
   "source": [
    "::::{prf:definition} \n",
    "\n",
    "The *drop in Gini impurity* for a splitting criterion $A$ on a dataset $D$ with respect to the class attribute is defined as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Delta \\Gini_{A}(D) &:= \\Gini(D) - \\Gini_{A}(D)\\\\\n",
    "\\Gini_{A}(D) &:= \\sum_{j} \\frac{|D_j|}{|D|} \\Gini(D_j),\n",
    "\\end{align}\n",
    "$$ (Delta-Gini)\n",
    "\n",
    "where $D$ is split by $A$ into $D_j$ for different outcomes $j$ of the split.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6c7352",
   "metadata": {},
   "source": [
    "We will consider the binary splitting criterion $\\R{X}\\leq s$ in particular, which gives\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Delta \\Gini_{A}(D) = g(\\hat{P}_\\R{Y}) - \\left[\\hat{P}[\\R{X}\\leq s] g(\\hat{p}_{\\R{Y}|\\R{X}\\leq s}) + \\hat{P}[\\R{X}> s]g(\\hat{p}_{\\R{Y}|\\R{X}> s})\\right]\n",
    "\\end{align}\n",
    "$$ (Delta-Gini-binary)\n",
    "\n",
    "where \n",
    "\n",
    "- $\\R{Y}$ denotes the target,\n",
    "- $\\hat{P}$ denotes the empirical distribution, and\n",
    "- $\\hat{p}_{\\R{Y}|\\R{X}\\leq s}$, $\\hat{p}_{\\R{Y}|\\R{X}> s}$, and $\\hat{p}_{\\R{Y}}$ denote the empirical probability mass functions of $\\R{Y}$ with or without conditioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02653b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_in_gini(X, Y, s):\n",
    "    \"\"\"\n",
    "    Calculate the drop in Gini impurity for a binary split.\n",
    "\n",
    "    This function computes the reduction in Gini impurity of the target `Y`\n",
    "    when the input feature `X` is split at the threshold `s`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 1D array-like\n",
    "        Input feature values for different instances.\n",
    "    Y : 1D array-like\n",
    "        Target values corresponding to `X`.\n",
    "    s : float\n",
    "        Splitting point for `X`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The reduction in Gini impurity resulting from the split.\n",
    "    \"\"\"\n",
    "    S = X <= s\n",
    "    q = S.mean()\n",
    "    return g(dist(Y)) - q * g(dist(Y[S])) - (1 - q) * g(dist(Y[~S]))\n",
    "\n",
    "\n",
    "X, Y = df[\"petal width (cm)\"], df.target\n",
    "print(f\"Drop in Gini: {drop_in_gini(X, Y, 0.8):.4g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6141cda9",
   "metadata": {},
   "source": [
    "To compute the best splitting point for a given input feature, we check every consecutive mid-points of the observed feature values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061ba08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split_pt(X, Y, gain_function):\n",
    "    \"\"\"\n",
    "    Find the best splitting point and the maximum gain for a binary split.\n",
    "\n",
    "    This function identifies the optimal splitting point `s` that maximizes the\n",
    "    gain as evaluated by the provided `gain_function` for the split `X <= s` \n",
    "    and target `Y`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 1D array-like\n",
    "        Input feature values for different instances.\n",
    "    Y : 1D array-like\n",
    "        Target values corresponding to `X`.\n",
    "    gain_function : function\n",
    "        A function that evaluates the gain for a splitting criterion `X <= s`.\n",
    "        For example, `drop_in_gini`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (s, g) where `s` is the best splitting point and `g` is the maximum \n",
    "        gain.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    drop_in_gini : Function to calculate the drop in Gini impurity.\n",
    "    \"\"\"\n",
    "    values = np.sort(np.unique(X))\n",
    "    split_pts = (values[1:] + values[:-1]) / 2\n",
    "    gain = np.array([gain_function(X, Y, s) for s in split_pts])\n",
    "    i = np.argmax(gain)\n",
    "    return split_pts[i], gain[i]\n",
    "\n",
    "\n",
    "print(\n",
    "    \"\"\"Best split point: {0:.3g}\n",
    "Maximum gain: {1:.3g}\"\"\".format(\n",
    "        *find_best_split_pt(X, Y, drop_in_gini)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454ffc0b",
   "metadata": {},
   "source": [
    "The following ranks the features according to the gains of their best binary splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca636e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_by_gini = pd.DataFrame(\n",
    "    {\n",
    "        \"feature\": feature,\n",
    "        **(lambda s, g: {\"split point\": s, \"gain\": g})(\n",
    "            *find_best_split_pt(df[feature], df.target, drop_in_gini)\n",
    "        ),\n",
    "    }\n",
    "    for feature in iris.feature_names\n",
    ").sort_values(by=\"gain\", ascending=False)\n",
    "rank_by_gini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc71fc4",
   "metadata": {},
   "source": [
    "Using the entropy to measure impurity, we have the following alternative gain function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63e7e62",
   "metadata": {},
   "source": [
    "::::{prf:definition} information gain\n",
    "\n",
    "The information gain is defined as \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Gain_{A}(D) &:= \\Info(D) - \\Info_{A}(D) && \\text{where}\\\\\n",
    "\\Info_{A}(D) &:= \\sum_{j} \\frac{|D_j|}{|D|} \\Info(D_j),\n",
    "\\end{align}\n",
    "$$ (Gain)\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fecf567",
   "metadata": {},
   "source": [
    "We will again consider the binary splitting criterion $\\R{X}\\leq s$ in particular, which gives\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Gain_{\\R{X}\\leq s}(D) = h(\\hat{P}_Y) - \\left[\\hat{P}[\\R{X}\\leq s] h(\\hat{p}_{\\R{Y}|\\R{X}\\leq s}) + \\hat{P}[\\R{X}> s]h(\\hat{p}_{\\R{Y}|\\R{X}> s})\\right]\n",
    "\\end{align}\n",
    "$$ (Gain-binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e2d09f",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:gain\n",
    "\n",
    "Complete the following function to calculate the information gain on the target $\\R{Y}$ for a binary split $\\R{X}\\leq s$. You may use `dist` and `h` defined previously.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5076d8fb",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b78a3f2a570846b18dfb3cf5426109e7",
     "grade": false,
     "grade_id": "info-gain",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "def gain(X, Y, s):\n",
    "    \"\"\"\n",
    "    Calculate the information gain for a binary split.\n",
    "\n",
    "    This function computes the information gain of the target `Y` when the \n",
    "    input feature `X` is split at the threshold `s`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 1D array-like\n",
    "        Input feature values for different instances.\n",
    "    Y : 1D array-like\n",
    "        Target values corresponding to `X`.\n",
    "    s : float\n",
    "        Splitting point for `X`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The information gain resulting from the split.\n",
    "    \"\"\"\n",
    "    S = X <= s\n",
    "    q = S.mean()\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "print(f\"Information gain: {gain(X, Y, 0.8):.4g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da37b3b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f3c3c5aaf0914408827891a35c41d00b",
     "grade": true,
     "grade_id": "test-info-gain",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-output",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# tests\n",
    "rank_by_entropy = pd.DataFrame(\n",
    "    {\n",
    "        \"feature\": feature,\n",
    "        **(lambda s, g: {\"split point\": s, \"gain\": g})(\n",
    "            *find_best_split_pt(df[feature], df.target, gain)\n",
    "        ),\n",
    "    }\n",
    "    for feature in iris.feature_names\n",
    ").sort_values(by=\"gain\", ascending=False)\n",
    "rank_by_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9bb77a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ab4b700bdec4cc39eb845394b3d641c",
     "grade": true,
     "grade_id": "htest-info-gain",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# hidden tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d821be",
   "metadata": {},
   "source": [
    "The C4.5 tree induction algorithm uses information gain ratio instead of information gain:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a139f2",
   "metadata": {},
   "source": [
    "::::{prf:definition} gain ratio\n",
    "\n",
    "The *information gain ratio* is defined as \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\GainRatio_{A}(D) &:= \\frac{\\Gain_{A}(D)}{\\operatorname{SplitInfo}_{A}(D)}\n",
    "\\end{align}\n",
    "$$ (GainRatio)\n",
    "\n",
    "which is normalized by \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\operatorname{SplitInfo}_{A}(D) &:= h\\left(j\\mapsto \\frac{|D_j|}{|D|} \\right)=\\sum_j \\frac{|D_j|}{|D|}\\log \\frac{|D|}{|D_j|}.\n",
    "\\end{align}\n",
    "$$ (SplitInfo)\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dfa674",
   "metadata": {},
   "source": [
    "For binary split $\\R{X}\\leq s$,\n",
    "\n",
    "$$\n",
    "\\operatorname{SplitInfo}_{\\R{X}\\leq s}(D) := h\\left(\\hat{P}[\\R{X}\\leq s], \\hat{P}[\\R{X}> s]\\right)\n",
    "$$ (SplitInfo-binary)\n",
    "\n",
    "in terms of the empirical distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5579840",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:gain_ratio\n",
    "\n",
    "Complete the following function to calculate the *information gain ratio* for a binary split $\\R{X}\\leq s$ and target $\\R{Y}$.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca51a49",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea4f85a7fec4499b5a2fd7eb6d10dfb1",
     "grade": false,
     "grade_id": "info-gain-ratio",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "def gain_ratio(X, Y, split_pt): \n",
    "    # Add docstring here\n",
    "    S = X <= split_pt\n",
    "    q = S.mean()\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ab6321",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3b1c1b36da376fcc022dfd4963ceff8f",
     "grade": true,
     "grade_id": "test-info-gain-ratio",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-output",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# tests\n",
    "rank_by_gain_ratio = pd.DataFrame(\n",
    "    {\n",
    "        \"feature\": feature,\n",
    "        **(lambda s, g: {\"split point\": s, \"gain\": g})(\n",
    "            *find_best_split_pt(df[feature], df.target, gain_ratio)\n",
    "        ),\n",
    "    }\n",
    "    for feature in iris.feature_names\n",
    ").sort_values(by=\"gain\", ascending=False)\n",
    "rank_by_gain_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2d5d19",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2cdd7f45cef622a317fd93f4d8db3c4c",
     "grade": true,
     "grade_id": "htest-info-gain-ratio",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# hidden tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a968dc2",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:difference\n",
    "\n",
    "Does the information gain ratio give a different ranking of the features in the above? Why?\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0e7686",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d13962fbe953658626e1dc57921795e0",
     "grade": true,
     "grade_id": "difference",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "default_lexer": "ipython3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
